{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pymysql\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from lxml import etree\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Constants</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_DISTANCE = 2\n",
    "\n",
    "RE_E = re.compile('ё', re.U)\n",
    "RE_S = re.compile('\\s+', re.U)\n",
    "\n",
    "REDUNDANT_TAGS = ['Geox', 'Orgn', 'Trad', 'Qual', 'perf', 'impf', 'pres', 'past', 'futr', 'incl', 'excl', 'Infr',\n",
    "                  'Slng', 'Arch', 'Litr', 'Inmx', 'Vpre', 'LATN', 'NUMB', 'SYMB', 'UNKN']\n",
    "\n",
    "KEYBOARD = {'a': 'qwedcxz`', 'b': 'vfghn', 'c': 'xsdfv', 'd': 'xswerfvc', 'e': 'sw234rfd', 'f': 'cdertgbv',\n",
    "            'g': 'vfrtyhnb', 'h': 'bgtyujmn', 'i': 'ju789olk', 'j': 'nhyuik,m', 'k': 'mjuiol.,', 'l': ',kiop;/.',\n",
    "            'm': 'nhjk,', 'n': 'bghjm', 'o': 'ki890p;l', 'p': 'lo90-[\\';', 'q': '12wsa', 'r': 'de345tgf',\n",
    "            's': 'zaqwedcx', 't': 'fr456yhg', 'u': 'hy678ikj', 'v': 'cdfgb', 'w': 'aq123eds', 'x': 'zasdc',\n",
    "            'y': 'gt567ujh', 'z': '`asx', 'а': 'свукепим', 'б': 'ьолдю', 'в': 'чыцукамс', 'г': 'рн678шло',\n",
    "            'д': 'блшщзж/ю', 'е': 'ак456нрп', 'ж': 'юдщзхэ/', 'з': 'дщ90-хэж', 'и': 'мапрт', 'й': '12цыф',\n",
    "            'к': 'ву345епа', 'л': 'ьогшщдюб', 'м': 'свапи', 'н': 'пе567гор', 'о': 'трнгшлбь', 'п': 'макенрти',\n",
    "            'р': 'ипенгоьт', 'с': 'чывам', 'т': 'ипроь', 'у': 'ыц234кав', 'ф': 'йцычя', 'х': 'жз0-=ъеэ',\n",
    "            'ц': 'й123увыф', 'ч': 'яфывс', 'ш': 'ог789щдл', 'щ': 'лш890зжд', 'ъ': 'эх-=е', 'ы': 'яфйцувсч',\n",
    "            'ь': 'тролб', 'э': '/жзхъе', 'ю': 'блдж/', 'я': 'фыч'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MySQL connector and opencorpora loader</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_conn():\n",
    "    return pymysql.connect(\n",
    "        host='127.0.0.1',\n",
    "        unix_socket='/tmp/mysql.sock',\n",
    "        user='root',\n",
    "        passwd=None,\n",
    "        db='spellcheck',\n",
    "        charset='utf8'\n",
    "    )\n",
    "\n",
    "\n",
    "def load_corpora(db=False):\n",
    "    # lemma = {\n",
    "    #     id: {\n",
    "    #         'text': '',\n",
    "    #         'gram': [\n",
    "    #             '',\n",
    "    #             ...\n",
    "    #         ],\n",
    "    #         'par': [\n",
    "    #             {\n",
    "    #                 'text': '',\n",
    "    #                 'gram': [\n",
    "    #                     '',\n",
    "    #                     ...\n",
    "    #                 ]\n",
    "    #             },\n",
    "    #             ...\n",
    "    #         ]\n",
    "    #     }\n",
    "    # }\n",
    "    lemma = defaultdict(dict)\n",
    "\n",
    "    xml_iter = etree.iterparse('dict.opcorpora.xml', events=('start', 'end'))\n",
    "\n",
    "    conn = get_conn()\n",
    "    c = conn.cursor()\n",
    "    if db:\n",
    "        c.execute(\"\"\"\n",
    "            DROP TABLE IF EXISTS `spellcheck`.`word_form`;\n",
    "            DROP TABLE IF EXISTS `spellcheck`.`lemma`;\n",
    "            CREATE TABLE `spellcheck`.`lemma` (\n",
    "              `id`   INT(11)     NOT NULL,\n",
    "              `gram` VARCHAR(50) NOT NULL,\n",
    "              PRIMARY KEY (`id`)\n",
    "            )\n",
    "              ENGINE InnoDB\n",
    "              CHARACTER SET utf8;\n",
    "            CREATE TABLE `spellcheck`.`word_form` (\n",
    "              `id`       INT(11)      NOT NULL AUTO_INCREMENT,\n",
    "              `lemma_id` INT(11)      NOT NULL,\n",
    "              `text`     VARCHAR(37)  NOT NULL,\n",
    "              `length`   SMALLINT     NOT NULL DEFAULT 0,\n",
    "              `gram`     VARCHAR(50)  NOT NULL,\n",
    "              PRIMARY KEY (`id`),\n",
    "              INDEX `lemma_id_idx` (`lemma_id` ASC),\n",
    "              CONSTRAINT `lemma_id`\n",
    "              FOREIGN KEY (`lemma_id`)\n",
    "              REFERENCES `spellcheck`.`lemma` (`id`)\n",
    "                ON DELETE NO ACTION\n",
    "                ON UPDATE NO ACTION\n",
    "            )\n",
    "              ENGINE InnoDB\n",
    "              CHARACTER SET utf8;\n",
    "        \"\"\")\n",
    "\n",
    "    while True:\n",
    "        act, it = xml_iter.__next__()\n",
    "\n",
    "        if act == 'start' and it.tag == 'lemma':\n",
    "            _id = int(it.attrib['id'])\n",
    "            \n",
    "            if not _id % 50000:  # for debug usages (total approx 400k)\n",
    "                print(_id)\n",
    "            \n",
    "            # retrieve lemma and its paradigm data\n",
    "            act, it = xml_iter.__next__()  # <l t=\"\">\n",
    "\n",
    "            lemma[_id]['text'] = RE_E.sub('е', it.attrib['t'])\n",
    "            lemma[_id]['gram'] = []\n",
    "            lemma[_id]['par'] = []\n",
    "\n",
    "            # retrieve lemma grams\n",
    "            while True:\n",
    "                act, it = xml_iter.__next__()  # <g v=\"\">\n",
    "                if act == 'start' and it.tag == 'g':\n",
    "                    lemma[_id]['gram'].append(it.attrib['v'])\n",
    "                    continue\n",
    "                if act == 'end' and it.tag == 'l':\n",
    "                    break\n",
    "\n",
    "            # retrieve lemma paradigm\n",
    "            while True:\n",
    "                act, it = xml_iter.__next__()  # <f t=\"\">\n",
    "                if act == 'start' and it.tag == 'f':\n",
    "                    # retrieve word form grams\n",
    "                    wf = {'text': RE_E.sub('е', it.attrib['t']), 'gram': []}\n",
    "                    while True:\n",
    "                        act, it = xml_iter.__next__()  # <g v=\"\">\n",
    "                        if act == 'start' and it.tag == 'g':\n",
    "                            wf['gram'].append(it.attrib['v'])\n",
    "                            continue\n",
    "                        if act == 'end' and it.tag == 'f':\n",
    "                            break\n",
    "                    lemma[_id]['par'].append(wf)\n",
    "                if act == 'end' and it.tag == 'lemma':\n",
    "                    break\n",
    "\n",
    "            if db:\n",
    "                c.execute(\"\"\"INSERT INTO lemma VALUES (%s, \"%s\")\"\"\" % (_id, ','.join(lemma[_id]['gram'])))\n",
    "                c.execute(\"\"\"INSERT INTO word_form (lemma_id, text, length, gram) VALUES \"\"\" + ','.join([\"\"\"(%s, \"%s\", %s, \"%s\")\"\"\" % (_id, wf['text'], len(wf['text']), ','.join(wf['gram'])) for wf in lemma[_id]['par']]))\n",
    "\n",
    "        if act == 'end' and it.tag == 'lemmata':\n",
    "            break\n",
    "\n",
    "    if db:\n",
    "        c.execute(\"\"\"\n",
    "            ALTER TABLE `spellcheck`.`word_form`\n",
    "                ADD INDEX `text` (`text` ASC);\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PrefixTree class and fuzzy_match function</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrefixTree(object):\n",
    "    def __init__(self, char='', parent=None):\n",
    "        self.char = char\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.is_word = False\n",
    "\n",
    "    def trace(self):\n",
    "        return (self.parent.trace() if self.parent is not None else '') + self.char\n",
    "\n",
    "    def _to_list(self):\n",
    "        if self.is_word:\n",
    "            yield self.trace()\n",
    "        for pt in self.children.values():\n",
    "            for s in pt._to_list():\n",
    "                yield s\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._to_list()\n",
    "\n",
    "    def __contains__(self, value):\n",
    "        if not value:\n",
    "            return True\n",
    "\n",
    "        if value[0] not in self.children:\n",
    "            return False\n",
    "\n",
    "        return value[1:] in self.children[value[0]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parent) + 1 if self.parent is not None else 0\n",
    "\n",
    "    def insert(self, value):\n",
    "        if not value:\n",
    "            self.is_word = True\n",
    "            return\n",
    "\n",
    "        c = value[0]\n",
    "        if c not in self.children:\n",
    "            self.children[c] = PrefixTree(c, self)\n",
    "\n",
    "        self.children[c].insert(value[1:])\n",
    "\n",
    "\n",
    "def load_ptree(from_file=True):\n",
    "    \"\"\"\n",
    "    Creates PrefixTree from corpora stored in DB or loads it from pickle serialization file\n",
    "    \"\"\"\n",
    "    if from_file:\n",
    "        with open('pt.pkl', mode='rb') as pt_pkl:\n",
    "            pt = pickle.load(pt_pkl)\n",
    "        return pt\n",
    "\n",
    "    pt = PrefixTree()\n",
    "    conn = get_conn()\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT text FROM word_form\")\n",
    "\n",
    "    inserted = 0  # for debug usages (total approx 5kk)\n",
    "    for row in c:\n",
    "        pt.insert(row[0])\n",
    "        inserted += 1\n",
    "        if not inserted % 1000000:\n",
    "            print(inserted)\n",
    "\n",
    "    with open('pt.pkl', mode='wb') as pt_pkl:\n",
    "        pickle.dump(pt, pt_pkl)\n",
    "\n",
    "    return pt\n",
    "\n",
    "def update_visited(ptree, visited):\n",
    "    \"\"\"\n",
    "    Removes one-word branch starting from leaf, going up to root node, ending in first branching node\n",
    "    \"\"\"\n",
    "    visited[ptree][-1] = 0\n",
    "    t = ptree.parent\n",
    "\n",
    "    while t is not None:\n",
    "        if len(t.children) != 1:\n",
    "            return\n",
    "        visited[t][-1] = 0\n",
    "        t = t.parent\n",
    "\n",
    "\n",
    "def is_visited(i, ptree, k, visited):\n",
    "    \"\"\"\n",
    "    Checks whether current node was visited within less operations (insert/delete/substitution/transposition)\n",
    "    \"\"\"\n",
    "    d = visited[ptree]\n",
    "    if -1 in d:  # -1 stands for \"node processed completely\"\n",
    "        return True\n",
    "\n",
    "    m = d.get(i, -1)  # get last distance value for string idx i\n",
    "    if k > m:\n",
    "        # proceed further if we came in this node for less operations (current k > last visit k)\n",
    "        d[i] = k\n",
    "        visited[ptree] = d\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def fuzzy_match(s, ptree, k, i=0, visited=None, n=0):\n",
    "    \"\"\"\n",
    "    Computes all strings contained in ptree with a distance <= k\n",
    "    \"\"\"\n",
    "    res = set()\n",
    "\n",
    "    # handles root node of a ptree\n",
    "    if ptree.parent is None and ptree.children:\n",
    "        n = len(s)\n",
    "        s += '\\0' * (k + 1)  # in order to leave an opportunity to insert chars into s\n",
    "        visited = defaultdict(dict)\n",
    "        for child in ptree.children.values():\n",
    "            # main loop, process each starting char in a prefix tree\n",
    "            res.update(fuzzy_match(s, child, k, i, visited, n))\n",
    "        return res\n",
    "    \n",
    "    # already tried\n",
    "    if is_visited(i, ptree, k, visited):\n",
    "        return []\n",
    "\n",
    "    # can't match\n",
    "    if k == -1 or (k == 0 and s[i] != ptree.char):\n",
    "        return []\n",
    "\n",
    "    if ptree.is_word and (n - i <= k or (n - (i + 1) <= k and ptree.char == s[i])):\n",
    "        res.add(ptree.trace())\n",
    "        if not ptree.children:\n",
    "            update_visited(ptree, visited)\n",
    "            return res\n",
    "\n",
    "    if ptree.char != s[i]:\n",
    "        res.update(fuzzy_match(s, ptree, k - 1, i + 1, visited, n))  # insert s char\n",
    "\n",
    "    for child in ptree.children.values():\n",
    "        if n >= i + 2 and s[i + 1] == ptree.char and s[i] == child.char:  # transposition\n",
    "            if child.is_word and k == 1 and n == i + 2:\n",
    "                # following transition to grandchild omits the case (in current architecture)\n",
    "                # when child node forms a valid trace, check it (in upper if) and append manually\n",
    "                res.add(child.trace())\n",
    "                if not child.children:\n",
    "                    update_visited(child, visited)\n",
    "\n",
    "            for grandchild in child.children.values():\n",
    "                res.update(fuzzy_match(s, grandchild, k - 1, i + 2, visited, n))\n",
    "\n",
    "        if ptree.char == s[i]:\n",
    "            res.update(fuzzy_match(s, child, k, i + 1, visited, n))  # chars are matched, k remains the same\n",
    "        else:\n",
    "            res.update(fuzzy_match(s, child, k - 1, i + 1, visited, n))  # substitution\n",
    "\n",
    "        res.update(fuzzy_match(s, child, k - 1, i, visited, n))  # delete candidate char\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Weightened Damerau-Levenshtein distance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_dam_lev(a, b):\n",
    "    d = {}\n",
    "    for i in range(-1, len(a)):\n",
    "        d[(i, -1)] = i + 1\n",
    "    for j in range(-1, len(b)):\n",
    "        d[(-1, j)] = j + 1\n",
    "\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            subst_cost = trans_cost = 0\n",
    "            if a[i] != b[j]:\n",
    "                subst_cost = .8 if b[j] in KEYBOARD[a[i]] or a[i] in KEYBOARD[b[j]] else 1\n",
    "                trans_cost = .9\n",
    "\n",
    "            d[(i, j)] = min([\n",
    "                d[(i - 1, j)] + 1,  # deletion\n",
    "                d[(i, j - 1)] + 1,  # insertion\n",
    "                d[(i - 1, j - 1)] + subst_cost  # substitution\n",
    "            ])\n",
    "\n",
    "            if i and j and a[i] == b[j - 1] and a[i - 1] == b[j]:\n",
    "                d[(i, j)] = min([\n",
    "                    d[(i, j)],\n",
    "                    d[(i - 2, j - 2)] + trans_cost\n",
    "                ])\n",
    "\n",
    "    return d[(len(a) - 1, len(b) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filtering candidates according to context (within tags N-grams counts) and Damerau-Levenshtein distance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_tags(word, morph):\n",
    "    if not word:\n",
    "        return ['PNCT']\n",
    "\n",
    "    res = {}\n",
    "    for var in morph.parse(word):\n",
    "        res[','.join(sorted([t for t in RE_S.sub(',', str(var.tag)).split(',') if t not in REDUNDANT_TAGS]))] = var.score\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_ngram_relevance_score(left_t, c_t, right_t, c_t_pool, bi_cnt, tri_cnt):\n",
    "    l_total = r_total = s_total = 0\n",
    "\n",
    "    for t in set(c_t_pool):\n",
    "        l_total += bi_cnt.get('%s+%s' % (left_t, t), 0)\n",
    "        r_total += bi_cnt.get('%s+%s' % (t, right_t), 0)\n",
    "        s_total += tri_cnt.get('%s+%s+%s' % (left_t, t, right_t), 0)\n",
    "\n",
    "    l = 0 if not l_total else float(bi_cnt.get('%s+%s' % (left_t, c_t), 0)) / l_total\n",
    "    r = 0 if not r_total else float(bi_cnt.get('%s+%s' % (c_t, right_t), 0)) / r_total\n",
    "    s = 0 if not s_total else float(tri_cnt.get('%s+%s+%s' % (left_t, c_t, right_t), 0)) / s_total\n",
    "\n",
    "    return .25 * l + .25 * r + .5 * s\n",
    "\n",
    "\n",
    "def filter_candidates(left, candidates, right, morph, bi_cnt, tri_cnt):\n",
    "    \"\"\"\n",
    "    1. Loop through each combination of left-right tags.\n",
    "       On each iteration we are trying to find the most relevant tag among each candidate tag.\n",
    "            2. Relevance is measured as weighted sum of bi/trigram probabilities counted by maximum likelihood\n",
    "               only among candidates tags.\n",
    "            3. Relevance is multiplied on a probability of the candidate to belong to concrete tag.\n",
    "            4. Relevance is multiplied on the probabilities of left/right word tags\n",
    "            5. Compute log of relevance score and multiply it on (candidate_dam_lev_distance + 1) of candidate.\n",
    "    6. Repeat 2-5 for each combination of left-right tags.\n",
    "    7. Find maximum of values computed in 5 and argmax (which is the desired candidate).\n",
    "\n",
    "    :param left:        left word or None in case of sentence beginning\n",
    "    :param candidates:  dictionary of candidates {'candidate': candidate_dam_lev_distance}\n",
    "    :param right:       right word or None in case of sentence ending\n",
    "    :param morph:       instance of MorphAnalyzer\n",
    "    :param bi_cnt:      dictionary of tags bigrams counts\n",
    "    :param tri_cnt:     dictionary of tags trigrams counts\n",
    "    :return:            most possible candidate\n",
    "    \"\"\"\n",
    "    all_freq = {}\n",
    "\n",
    "    tags_l = get_word_tags(left, morph)\n",
    "    tags_r = get_word_tags(right, morph)\n",
    "    tags_c = {}\n",
    "    c_t_pool = set()  # pool of distinct candidates tags\n",
    "\n",
    "    for c in candidates:\n",
    "        tags_c[c] = get_word_tags(c, morph)\n",
    "        for t, score in tags_c[c].items():\n",
    "            c_t_pool.add(t)\n",
    "\n",
    "    for lt in tags_l:\n",
    "        for rt in tags_r:\n",
    "            for c in candidates:\n",
    "                for ct in tags_c[c]:\n",
    "                    rel_score = np.log(get_ngram_relevance_score(lt, ct, rt, c_t_pool, bi_cnt, tri_cnt))\n",
    "                    rel_score += np.log(tags_c[c][ct])\n",
    "                    rel_score += np.log(tags_l[lt]) + np.log(tags_r[rt])\n",
    "                    rel_score *= candidates[c]\n",
    "\n",
    "                    all_freq['%s|%s:%s|%s' % (lt, c, ct, rt)] = rel_score\n",
    "\n",
    "    print('\\n'.join([str(x) for x in sorted(all_freq.items(), key=lambda x: x[1], reverse=True)]))  # for debug usages\n",
    "\n",
    "    # Each candidate tag for each candidate with relevance score\n",
    "    # for each left/right tag combination is stored in all_freq for now. Retrieve the best sequence\n",
    "    most_relevant_seq = max(all_freq, key=lambda k: all_freq[k])\n",
    "\n",
    "    # retrieve candidate from best sequence\n",
    "    return most_relevant_seq.split('|')[1].split(':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PrefixTree loading from pickle</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 36.91286087036133s\n"
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "pt = load_ptree()\n",
    "print(\"Loading time: %ss\" % (time() - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Counts of OpenCorpora tags N-grams loading from json</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 0.0836181640625s\n"
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "tags_bi = json.load(open('bigram.opcorpora.json', encoding='utf-8'))\n",
    "tags_tri = json.load(open('trigram.opcorpora.json', encoding='utf-8'))\n",
    "print(\"Loading time: %ss\" % (time() - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Some testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.00010633468627929688s\n",
      "Exists: False\n",
      "\n",
      "Execution time: 0.17548584938049316s\n",
      "Candidates count: 11\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b8c801a95e79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'повести'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'на'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_bi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_tri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nExecution time: %ss\\n'%s %s %s' ===> '%s %s %s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e13d80df29f5>\u001b[0m in \u001b[0;36mfilter_candidates\u001b[0;34m(left, candidates, right, morph, bi_cnt, tri_cnt)\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0mrel_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mrel_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0mrel_score\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mall_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%s|%s:%s|%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "s = 'прикраснее'\n",
    "res = s in pt\n",
    "print(\"Execution time: %ss\\nExists: %s\" % (time() - st, res))\n",
    "\n",
    "st = time()\n",
    "res = fuzzy_match(s, pt, 2)\n",
    "print(\"\\nExecution time: %ss\\nCandidates count: %s\" % (time() - st, len(res)))\n",
    "\n",
    "ma = MorphAnalyzer()\n",
    "st = time()\n",
    "l = 'повести'\n",
    "r = 'на'\n",
    "res = filter_candidates(l, res, r, ma, tags_bi, tags_tri)\n",
    "print(\"\\nExecution time: %ss\\n'%s %s %s' ===> '%s %s %s'\" % (time() - st, l, s, r, l, res, r))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
