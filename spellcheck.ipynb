{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from lxml import etree\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Constants</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_DISTANCE = 2\n",
    "\n",
    "RE_E = re.compile('ั', re.U)\n",
    "RE_S = re.compile('\\s+', re.U)\n",
    "\n",
    "REDUNDANT_TAGS = ['Geox', 'Orgn', 'Trad', 'Qual', 'perf', 'impf', 'pres', 'past', 'futr', 'incl', 'excl', 'Infr',\n",
    "                  'Slng', 'Arch', 'Litr', 'Inmx', 'Vpre', 'LATN', 'NUMB', 'SYMB', 'UNKN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MySQL connector and opencorpora loader</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_conn():\n",
    "    return pymysql.connect(\n",
    "        host='127.0.0.1',\n",
    "        unix_socket='/tmp/mysql.sock',\n",
    "        user='root',\n",
    "        passwd=None,\n",
    "        db='spellcheck',\n",
    "        charset='utf8'\n",
    "    )\n",
    "\n",
    "\n",
    "def load_corpora(db=False):\n",
    "    # lemma = {\n",
    "    #     id: {\n",
    "    #         'text': '',\n",
    "    #         'gram': [\n",
    "    #             '',\n",
    "    #             ...\n",
    "    #         ],\n",
    "    #         'par': [\n",
    "    #             {\n",
    "    #                 'text': '',\n",
    "    #                 'gram': [\n",
    "    #                     '',\n",
    "    #                     ...\n",
    "    #                 ]\n",
    "    #             },\n",
    "    #             ...\n",
    "    #         ]\n",
    "    #     }\n",
    "    # }\n",
    "    lemma = defaultdict(dict)\n",
    "\n",
    "    xml_iter = etree.iterparse('dict.opcorpora.xml', events=('start', 'end'))\n",
    "\n",
    "    conn = get_conn()\n",
    "    c = conn.cursor()\n",
    "    if db:\n",
    "        c.execute(\"\"\"\n",
    "            DROP TABLE IF EXISTS `spellcheck`.`word_form`;\n",
    "            DROP TABLE IF EXISTS `spellcheck`.`lemma`;\n",
    "            CREATE TABLE `spellcheck`.`lemma` (\n",
    "              `id`   INT(11)     NOT NULL,\n",
    "              `gram` VARCHAR(50) NOT NULL,\n",
    "              PRIMARY KEY (`id`)\n",
    "            )\n",
    "              ENGINE InnoDB\n",
    "              CHARACTER SET utf8;\n",
    "            CREATE TABLE `spellcheck`.`word_form` (\n",
    "              `id`       INT(11)      NOT NULL AUTO_INCREMENT,\n",
    "              `lemma_id` INT(11)      NOT NULL,\n",
    "              `text`     VARCHAR(37)  NOT NULL,\n",
    "              `length`   SMALLINT     NOT NULL DEFAULT 0,\n",
    "              `gram`     VARCHAR(50)  NOT NULL,\n",
    "              PRIMARY KEY (`id`),\n",
    "              INDEX `lemma_id_idx` (`lemma_id` ASC),\n",
    "              CONSTRAINT `lemma_id`\n",
    "              FOREIGN KEY (`lemma_id`)\n",
    "              REFERENCES `spellcheck`.`lemma` (`id`)\n",
    "                ON DELETE NO ACTION\n",
    "                ON UPDATE NO ACTION\n",
    "            )\n",
    "              ENGINE InnoDB\n",
    "              CHARACTER SET utf8;\n",
    "        \"\"\")\n",
    "\n",
    "    while True:\n",
    "        act, it = xml_iter.__next__()\n",
    "\n",
    "        if act == 'start' and it.tag == 'lemma':\n",
    "            _id = int(it.attrib['id'])\n",
    "            \n",
    "            if not _id % 50000:  # for debug usages (total approx 400k)\n",
    "                print(_id)\n",
    "            \n",
    "            # retrieve lemma and its paradigm data\n",
    "            act, it = xml_iter.__next__()  # <l t=\"\">\n",
    "\n",
    "            lemma[_id]['text'] = RE_E.sub('ะต', it.attrib['t'])\n",
    "            lemma[_id]['gram'] = []\n",
    "            lemma[_id]['par'] = []\n",
    "\n",
    "            # retrieve lemma grams\n",
    "            while True:\n",
    "                act, it = xml_iter.__next__()  # <g v=\"\">\n",
    "                if act == 'start' and it.tag == 'g':\n",
    "                    lemma[_id]['gram'].append(it.attrib['v'])\n",
    "                    continue\n",
    "                if act == 'end' and it.tag == 'l':\n",
    "                    break\n",
    "\n",
    "            # retrieve lemma paradigm\n",
    "            while True:\n",
    "                act, it = xml_iter.__next__()  # <f t=\"\">\n",
    "                if act == 'start' and it.tag == 'f':\n",
    "                    # retrieve word form grams\n",
    "                    wf = {'text': RE_E.sub('ะต', it.attrib['t']), 'gram': []}\n",
    "                    while True:\n",
    "                        act, it = xml_iter.__next__()  # <g v=\"\">\n",
    "                        if act == 'start' and it.tag == 'g':\n",
    "                            wf['gram'].append(it.attrib['v'])\n",
    "                            continue\n",
    "                        if act == 'end' and it.tag == 'f':\n",
    "                            break\n",
    "                    lemma[_id]['par'].append(wf)\n",
    "                if act == 'end' and it.tag == 'lemma':\n",
    "                    break\n",
    "\n",
    "            if db:\n",
    "                c.execute(\"\"\"INSERT INTO lemma VALUES (%s, \"%s\")\"\"\" % (_id, ','.join(lemma[_id]['gram'])))\n",
    "                c.execute(\"\"\"INSERT INTO word_form (lemma_id, text, length, gram) VALUES \"\"\" + ','.join([\"\"\"(%s, \"%s\", %s, \"%s\")\"\"\" % (_id, wf['text'], len(wf['text']), ','.join(wf['gram'])) for wf in lemma[_id]['par']]))\n",
    "\n",
    "        if act == 'end' and it.tag == 'lemmata':\n",
    "            break\n",
    "\n",
    "    if db:\n",
    "        c.execute(\"\"\"\n",
    "            ALTER TABLE `spellcheck`.`word_form`\n",
    "                ADD INDEX `text` (`text` ASC);\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PrefixTree class and fuzzy_match function</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrefixTree(object):\n",
    "    def __init__(self, char='', parent=None):\n",
    "        self.char = char\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.is_word = False\n",
    "\n",
    "    def trace(self):\n",
    "        return (self.parent.trace() if self.parent is not None else '') + self.char\n",
    "\n",
    "    def _to_list(self):\n",
    "        if self.is_word:\n",
    "            yield self.trace()\n",
    "        for pt in self.children.values():\n",
    "            for s in pt._to_list():\n",
    "                yield s\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._to_list()\n",
    "\n",
    "    def __contains__(self, value):\n",
    "        if not value:\n",
    "            return True\n",
    "\n",
    "        if value[0] not in self.children:\n",
    "            return False\n",
    "\n",
    "        return value[1:] in self.children[value[0]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parent) + 1 if self.parent is not None else 0\n",
    "\n",
    "    def insert(self, value):\n",
    "        if not value:\n",
    "            self.is_word = True\n",
    "            return\n",
    "\n",
    "        c = value[0]\n",
    "        if c not in self.children:\n",
    "            self.children[c] = PrefixTree(c, self)\n",
    "\n",
    "        self.children[c].insert(value[1:])\n",
    "\n",
    "\n",
    "def load_ptree(from_file=True):\n",
    "    \"\"\"\n",
    "    Creates PrefixTree from corpora stored in DB or loads it from pickle serialization file\n",
    "    \"\"\"\n",
    "    if from_file:\n",
    "        with open('pt.pkl', mode='rb') as pt_pkl:\n",
    "            pt = pickle.load(pt_pkl)\n",
    "        return pt\n",
    "\n",
    "    pt = PrefixTree()\n",
    "    conn = get_conn()\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT text FROM word_form\")\n",
    "\n",
    "    inserted = 0  # for debug usages (total approx 5kk)\n",
    "    for row in c:\n",
    "        pt.insert(row[0])\n",
    "        inserted += 1\n",
    "        if not inserted % 1000000:\n",
    "            print(inserted)\n",
    "\n",
    "    with open('pt.pkl', mode='wb') as pt_pkl:\n",
    "        pickle.dump(pt, pt_pkl)\n",
    "\n",
    "    return pt\n",
    "\n",
    "def update_visited(ptree, visited):\n",
    "    \"\"\"\n",
    "    Removes one-word branch starting from leaf, going up to root node, ending in first branching node\n",
    "    \"\"\"\n",
    "    visited[ptree][-1] = 0\n",
    "    t = ptree.parent\n",
    "\n",
    "    while t is not None:\n",
    "        if len(t.children) != 1:\n",
    "            return\n",
    "        visited[t][-1] = 0\n",
    "        t = t.parent\n",
    "\n",
    "\n",
    "def is_visited(i, ptree, k, visited):\n",
    "    \"\"\"\n",
    "    Checks whether current node was visited within less operations (insert/delete/substitution/transposition)\n",
    "    \"\"\"\n",
    "    d = visited[ptree]\n",
    "    if -1 in d:  # -1 stands for \"node processed completely\"\n",
    "        return True\n",
    "\n",
    "    m = d.get(i, -1)  # get last distance value for string idx i\n",
    "    if k > m:\n",
    "        # proceed further if we came in this node for less operations (current k > last visit k)\n",
    "        d[i] = k\n",
    "        visited[ptree] = d\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def fuzzy_match(s, ptree, k, i=0, visited=None, n=0):\n",
    "    \"\"\"\n",
    "    Computes all strings contained in ptree with a distance <= k\n",
    "    \"\"\"\n",
    "    res = set()\n",
    "\n",
    "    # handles root node of a ptree\n",
    "    if ptree.parent is None and ptree.children:\n",
    "        n = len(s)\n",
    "        s += '\\0' * (k + 1)  # in order to leave an opportunity to insert chars into s\n",
    "        visited = defaultdict(dict)\n",
    "        for child in ptree.children.values():\n",
    "            # main loop, process each starting char in a prefix tree\n",
    "            res.update(fuzzy_match(s, child, k, i, visited, n))\n",
    "        return res\n",
    "    \n",
    "    # already tried\n",
    "    if is_visited(i, ptree, k, visited):\n",
    "        return []\n",
    "\n",
    "    # can't match\n",
    "    if k == -1 or (k == 0 and s[i] != ptree.char):\n",
    "        return []\n",
    "\n",
    "    if ptree.is_word and (n - i <= k or (n - (i + 1) <= k and ptree.char == s[i])):\n",
    "        res.add(ptree.trace())\n",
    "        if not ptree.children:\n",
    "            update_visited(ptree, visited)\n",
    "            return res\n",
    "\n",
    "    if ptree.char != s[i]:\n",
    "        res.update(fuzzy_match(s, ptree, k - 1, i + 1, visited, n))  # insert s char\n",
    "\n",
    "    for child in ptree.children.values():\n",
    "        if n >= i + 2 and s[i + 1] == ptree.char and s[i] == child.char:  # transposition\n",
    "            if child.is_word and k == 1 and n == i + 2:\n",
    "                # following transition to grandchild omits the case (in current architecture)\n",
    "                # when child node forms a valid trace, check it (in upper if) and append manually\n",
    "                res.add(child.trace())\n",
    "                if not child.children:\n",
    "                    update_visited(child, visited)\n",
    "\n",
    "            for grandchild in child.children.values():\n",
    "                res.update(fuzzy_match(s, grandchild, k - 1, i + 2, visited, n))\n",
    "\n",
    "        if ptree.char == s[i]:\n",
    "            res.update(fuzzy_match(s, child, k, i + 1, visited, n))  # chars are matched, k remains the same\n",
    "        else:\n",
    "            res.update(fuzzy_match(s, child, k - 1, i + 1, visited, n))  # substitution\n",
    "\n",
    "        res.update(fuzzy_match(s, child, k - 1, i, visited, n))  # delete candidate char\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PrefixTree loading from pickle</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 38.78627300262451s\n"
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "\n",
    "pt = load_ptree()\n",
    "\n",
    "print(\"Loading time: %ss\" % (time() - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Counts of OpenCorpora tags N-grams loading from json</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 0.10080289840698242s\n"
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "\n",
    "tags_bi = json.load(open('bigram.opcorpora.json', encoding='utf-8'))\n",
    "tags_tri = json.load(open('trigram.opcorpora.json', encoding='utf-8'))\n",
    "\n",
    "print(\"Loading time: %ss\" % (time() - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filtering candidates according to context within tags N-grams counts</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_tags(word, morph):\n",
    "    if not word:\n",
    "        return ['PNCT']\n",
    "\n",
    "    res = []\n",
    "    for var in morph.parse(word):\n",
    "        res.append(','.join([t for t in RE_S.sub(',', str(var.tag)).split(',') if t not in REDUNDANT_TAGS]))\n",
    "\n",
    "    return res\n",
    "\n",
    "def filter_candidates(left, candidates, right, morph, counts_bi, counts_tri):\n",
    "    all_freq = {}\n",
    "\n",
    "    tags_left = get_word_tags(left, morph)\n",
    "    tags_right = get_word_tags(right, morph)\n",
    "\n",
    "    for c in candidates:\n",
    "        # TODO: use conditional probability\n",
    "        freq_l = freq_r = freq_tri = 0\n",
    "        for tc in get_word_tags(c, morph):\n",
    "            for tl in tags_left:\n",
    "                for tr in tags_right:\n",
    "                    freq_l += counts_bi.get('+'.join([tl, tc]), 0)\n",
    "                    freq_r += counts_bi.get('+'.join([tc, tr]), 0)\n",
    "                    freq_tri = counts_tri.get('+'.join([tl, tc, tr]), 0)\n",
    "\n",
    "        all_freq[c] = .25 * freq_l + .5 * freq_tri + .25 * freq_r\n",
    "    \n",
    "    print('\\n'.join([str(x) for x in sorted(all_freq.items(), key=lambda x: x[1], reverse=True)]))  # for debug usages\n",
    "\n",
    "    return max(all_freq, key=lambda k: all_freq[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Some testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.0001049041748046875s\n",
      "Exists: False\n",
      "\n",
      "Execution time: 0.18964719772338867s\n",
      "Candidates count: 11\n",
      "('ะฟัะธะบัะฐัะต', 400.25)\n",
      "('ะฟัะตะบัะฐัะฝัะต', 96.5)\n",
      "('ะฟัะตะบัะฐัะฝะพะต', 58.75)\n",
      "('ะฟัะตะบัะฐัะฝะตะต', 43.5)\n",
      "('ะฟัะธะทัะฐัะฝะตะต', 43.5)\n",
      "('ะฟัะธะบะปะฐะดะฝะตะต', 43.5)\n",
      "('ะฟัะธะบะฐะทะฝะตะต', 43.5)\n",
      "('ะฟัะธะบัะฐัััะต', 35.75)\n",
      "('ะฟัะธะบัะฐัะธัะต', 7.0)\n",
      "('ะฟะพะบัะฐัะฝะตะต', 3.75)\n",
      "('ะฟัะตะบัะฐัะฝะตะน', 3.5)\n",
      "\n",
      "Execution time: 0.008038997650146484s\n",
      "'ะฟะพะฒะตััะธ ะฟัะธะบัะฐัะฝะตะต ะฝะฐ' ===> 'ะฟะพะฒะตััะธ ะฟัะธะบัะฐัะต ะฝะฐ'\n"
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "s = 'ะฟัะธะบัะฐัะฝะตะต'\n",
    "res = s in pt\n",
    "print(\"Execution time: %ss\\nExists: %s\" % (time() - st, res))\n",
    "\n",
    "st = time()\n",
    "res = fuzzy_match(s, pt, 2)\n",
    "print(\"\\nExecution time: %ss\\nCandidates count: %s\" % (time() - st, len(res)))\n",
    "\n",
    "ma = MorphAnalyzer()\n",
    "st = time()\n",
    "l = 'ะฟะพะฒะตััะธ'\n",
    "r = 'ะฝะฐ'\n",
    "res = filter_candidates(l, res, r, ma, tags_bi, tags_tri)\n",
    "print(\"\\nExecution time: %ss\\n'%s %s %s' ===> '%s %s %s'\" % (time() - st, l, s, r, l, res, r))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
