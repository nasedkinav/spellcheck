{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pymorphy2\n",
    "import pymysql\n",
    "import pytils\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "from lxml import etree\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_DISTANCE = 2\n",
    "\n",
    "RE_E = re.compile('ё', re.U)\n",
    "RE_S = re.compile('\\s+', re.U)\n",
    "\n",
    "REDUNDANT_TAGS = ['Geox', 'Orgn', 'Trad', 'Qual', 'perf', 'impf', 'pres', 'past', 'futr', 'incl', 'excl', 'Infr',\n",
    "                  'Slng', 'Arch', 'Litr', 'Inmx', 'Vpre', 'LATN', 'NUMB', 'SYMB', 'UNKN']\n",
    "\n",
    "KEYBOARD = {'a': 'qwedcxz`', 'b': 'vfghn', 'c': 'xsdfv', 'd': 'xswerfvc', 'e': 'sw234rfd', 'f': 'cdertgbv',\n",
    "            'g': 'vfrtyhnb', 'h': 'bgtyujmn', 'i': 'ju789olk', 'j': 'nhyuik,m', 'k': 'mjuiol.,', 'l': ',kiop;/.',\n",
    "            'm': 'nhjk,', 'n': 'bghjm', 'o': 'ki890p;l', 'p': 'lo90-[\\';', 'q': '12wsa', 'r': 'de345tgf',\n",
    "            's': 'zaqwedcx', 't': 'fr456yhg', 'u': 'hy678ikj', 'v': 'cdfgb', 'w': 'aq123eds', 'x': 'zasdc',\n",
    "            'y': 'gt567ujh', 'z': '`asx', 'а': 'свукепим', 'б': 'ьолдю', 'в': 'чыцукамс', 'г': 'рн678шло',\n",
    "            'д': 'блшщзж/ю', 'е': 'ак456нрп', 'ж': 'юдщзхэ/', 'з': 'дщ90-хэж', 'и': 'мапрт', 'й': '12цыф',\n",
    "            'к': 'ву345епа', 'л': 'ьогшщдюб', 'м': 'свапи', 'н': 'пе567гор', 'о': 'трнгшлбь', 'п': 'макенрти',\n",
    "            'р': 'ипенгоьт', 'с': 'чывам', 'т': 'ипроь', 'у': 'ыц234кав', 'ф': 'йцычя', 'х': 'жз0-=ъеэ',\n",
    "            'ц': 'й123увыф', 'ч': 'яфывс', 'ш': 'ог789щдл', 'щ': 'лш890зжд', 'ъ': 'эх-=е', 'ы': 'яфйцувсч',\n",
    "            'ь': 'тролб', 'э': '/жзхъе', 'ю': 'блдж/', 'я': 'фыч'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL connector and opencorpora loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_conn():\n",
    "    return pymysql.connect(\n",
    "        host='127.0.0.1',\n",
    "        unix_socket='/tmp/mysql.sock',\n",
    "        user='root',\n",
    "        passwd=None,\n",
    "        db='spellcheck',\n",
    "        charset='utf8'\n",
    "    )\n",
    "\n",
    "\n",
    "def load_corpora(db=False):\n",
    "    # lemma = {\n",
    "    #     id: {\n",
    "    #         'text': '',\n",
    "    #         'gram': [\n",
    "    #             '',\n",
    "    #             ...\n",
    "    #         ],\n",
    "    #         'par': [\n",
    "    #             {\n",
    "    #                 'text': '',\n",
    "    #                 'gram': [\n",
    "    #                     '',\n",
    "    #                     ...\n",
    "    #                 ]\n",
    "    #             },\n",
    "    #             ...\n",
    "    #         ]\n",
    "    #     }\n",
    "    # }\n",
    "    lemma = defaultdict(dict)\n",
    "\n",
    "    xml_iter = etree.iterparse('dict.opcorpora.xml', events=('start', 'end'))\n",
    "\n",
    "    conn = get_conn()\n",
    "    c = conn.cursor()\n",
    "    if db:\n",
    "        c.execute(\"\"\"\n",
    "            DROP TABLE IF EXISTS `spellcheck`.`word_form`;\n",
    "            DROP TABLE IF EXISTS `spellcheck`.`lemma`;\n",
    "            CREATE TABLE `spellcheck`.`lemma` (\n",
    "              `id`   INT(11)     NOT NULL,\n",
    "              `gram` VARCHAR(50) NOT NULL,\n",
    "              PRIMARY KEY (`id`)\n",
    "            )\n",
    "              ENGINE InnoDB\n",
    "              CHARACTER SET utf8;\n",
    "            CREATE TABLE `spellcheck`.`word_form` (\n",
    "              `id`       INT(11)      NOT NULL AUTO_INCREMENT,\n",
    "              `lemma_id` INT(11)      NOT NULL,\n",
    "              `text`     VARCHAR(37)  NOT NULL,\n",
    "              `length`   SMALLINT     NOT NULL DEFAULT 0,\n",
    "              `gram`     VARCHAR(50)  NOT NULL,\n",
    "              PRIMARY KEY (`id`),\n",
    "              INDEX `lemma_id_idx` (`lemma_id` ASC),\n",
    "              CONSTRAINT `lemma_id`\n",
    "              FOREIGN KEY (`lemma_id`)\n",
    "              REFERENCES `spellcheck`.`lemma` (`id`)\n",
    "                ON DELETE NO ACTION\n",
    "                ON UPDATE NO ACTION\n",
    "            )\n",
    "              ENGINE InnoDB\n",
    "              CHARACTER SET utf8;\n",
    "        \"\"\")\n",
    "\n",
    "    while True:\n",
    "        act, it = xml_iter.__next__()\n",
    "\n",
    "        if act == 'start' and it.tag == 'lemma':\n",
    "            _id = int(it.attrib['id'])\n",
    "            \n",
    "            if not _id % 50000:  # for debug usages (total approx 400k)\n",
    "                print(_id)\n",
    "            \n",
    "            # retrieve lemma and its paradigm data\n",
    "            act, it = xml_iter.__next__()  # <l t=\"\">\n",
    "\n",
    "            lemma[_id]['text'] = RE_E.sub('е', it.attrib['t'])\n",
    "            lemma[_id]['gram'] = []\n",
    "            lemma[_id]['par'] = []\n",
    "\n",
    "            # retrieve lemma grams\n",
    "            while True:\n",
    "                act, it = xml_iter.__next__()  # <g v=\"\">\n",
    "                if act == 'start' and it.tag == 'g':\n",
    "                    lemma[_id]['gram'].append(it.attrib['v'])\n",
    "                    continue\n",
    "                if act == 'end' and it.tag == 'l':\n",
    "                    break\n",
    "\n",
    "            # retrieve lemma paradigm\n",
    "            while True:\n",
    "                act, it = xml_iter.__next__()  # <f t=\"\">\n",
    "                if act == 'start' and it.tag == 'f':\n",
    "                    # retrieve word form grams\n",
    "                    wf = {'text': RE_E.sub('е', it.attrib['t']), 'gram': []}\n",
    "                    while True:\n",
    "                        act, it = xml_iter.__next__()  # <g v=\"\">\n",
    "                        if act == 'start' and it.tag == 'g':\n",
    "                            wf['gram'].append(it.attrib['v'])\n",
    "                            continue\n",
    "                        if act == 'end' and it.tag == 'f':\n",
    "                            break\n",
    "                    lemma[_id]['par'].append(wf)\n",
    "                if act == 'end' and it.tag == 'lemma':\n",
    "                    break\n",
    "\n",
    "            if db:\n",
    "                c.execute(\"\"\"INSERT INTO lemma VALUES (%s, \"%s\")\"\"\" % (_id, ','.join(lemma[_id]['gram'])))\n",
    "                c.execute(\"\"\"INSERT INTO word_form (lemma_id, text, length, gram) VALUES \"\"\" + ','.join([\"\"\"(%s, \"%s\", %s, \"%s\")\"\"\" % (_id, wf['text'], len(wf['text']), ','.join(wf['gram'])) for wf in lemma[_id]['par']]))\n",
    "\n",
    "        if act == 'end' and it.tag == 'lemmata':\n",
    "            break\n",
    "\n",
    "    if db:\n",
    "        c.execute(\"\"\"\n",
    "            ALTER TABLE `spellcheck`.`word_form`\n",
    "                ADD INDEX `text` (`text` ASC);\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrefixTree class and fuzzy_match function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrefixTree(object):\n",
    "    def __init__(self, char='', parent=None):\n",
    "        self.char = char\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.is_word = False\n",
    "\n",
    "    def trace(self):\n",
    "        return (self.parent.trace() if self.parent is not None else '') + self.char\n",
    "\n",
    "    def _to_list(self):\n",
    "        if self.is_word:\n",
    "            yield self.trace()\n",
    "        for pt in self.children.values():\n",
    "            for s in pt._to_list():\n",
    "                yield s\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._to_list()\n",
    "\n",
    "    def __contains__(self, value):\n",
    "        if not value:\n",
    "            return True\n",
    "\n",
    "        if value[0] not in self.children:\n",
    "            return False\n",
    "\n",
    "        return value[1:] in self.children[value[0]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parent) + 1 if self.parent is not None else 0\n",
    "\n",
    "    def insert(self, value):\n",
    "        if not value:\n",
    "            self.is_word = True\n",
    "            return\n",
    "\n",
    "        c = value[0]\n",
    "        if c not in self.children:\n",
    "            self.children[c] = PrefixTree(c, self)\n",
    "\n",
    "        self.children[c].insert(value[1:])\n",
    "\n",
    "\n",
    "def load_ptree(from_file=True):\n",
    "    \"\"\"\n",
    "    Creates PrefixTree from corpora stored in DB or loads it from pickle serialization file\n",
    "    \"\"\"\n",
    "    if from_file:\n",
    "        with open('pt.pkl', mode='rb') as pt_pkl:\n",
    "            pt = pickle.load(pt_pkl)\n",
    "        return pt\n",
    "\n",
    "    pt = PrefixTree()\n",
    "    conn = get_conn()\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT text FROM word_form\")\n",
    "\n",
    "    inserted = 0  # for debug usages (total approx 5kk)\n",
    "    for row in c:\n",
    "        pt.insert(row[0])\n",
    "        inserted += 1\n",
    "        if not inserted % 1000000:\n",
    "            print(inserted)\n",
    "\n",
    "    with open('pt.pkl', mode='wb') as pt_pkl:\n",
    "        pickle.dump(pt, pt_pkl)\n",
    "\n",
    "    return pt\n",
    "\n",
    "def update_visited(ptree, visited):\n",
    "    \"\"\"\n",
    "    Removes one-word branch starting from leaf, going up to root node, ending in first branching node\n",
    "    \"\"\"\n",
    "    visited[ptree][-1] = 0\n",
    "    t = ptree.parent\n",
    "\n",
    "    while t is not None:\n",
    "        if len(t.children) != 1:\n",
    "            return\n",
    "        visited[t][-1] = 0\n",
    "        t = t.parent\n",
    "\n",
    "\n",
    "def is_visited(i, ptree, k, visited):\n",
    "    \"\"\"\n",
    "    Checks whether current node was visited within less operations (insert/delete/substitution/transposition)\n",
    "    \"\"\"\n",
    "    d = visited[ptree]\n",
    "    if -1 in d:  # -1 stands for \"node processed completely\"\n",
    "        return True\n",
    "\n",
    "    m = d.get(i, -1)  # get last distance value for string idx i\n",
    "    if k > m:\n",
    "        # proceed further if we came in this node for less operations (current k > last visit k)\n",
    "        d[i] = k\n",
    "        visited[ptree] = d\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def fuzzy_match(s, ptree, k, i=0, visited=None, n=0):\n",
    "    \"\"\"\n",
    "    Computes all strings contained in ptree with a distance <= k\n",
    "    \"\"\"\n",
    "    res = set()\n",
    "\n",
    "    # handles root node of a ptree\n",
    "    if ptree.parent is None and ptree.children:\n",
    "        n = len(s)\n",
    "        s += '\\0' * (k + 1)  # in order to leave an opportunity to insert chars into s\n",
    "        visited = defaultdict(dict)\n",
    "        for child in ptree.children.values():\n",
    "            # main loop, process each starting char in a prefix tree\n",
    "            res.update(fuzzy_match(s, child, k, i, visited, n))\n",
    "        return res\n",
    "    \n",
    "    # already tried\n",
    "    if is_visited(i, ptree, k, visited):\n",
    "        return []\n",
    "\n",
    "    # can't match\n",
    "    if k == -1 or (k == 0 and s[i] != ptree.char):\n",
    "        return []\n",
    "\n",
    "    if ptree.is_word and (n - i <= k or (n - (i + 1) <= k and ptree.char == s[i])):\n",
    "        res.add(ptree.trace())\n",
    "        if not ptree.children:\n",
    "            update_visited(ptree, visited)\n",
    "            return res\n",
    "\n",
    "    if ptree.char != s[i]:\n",
    "        res.update(fuzzy_match(s, ptree, k - 1, i + 1, visited, n))  # insert s char\n",
    "\n",
    "    for child in ptree.children.values():\n",
    "        if n >= i + 2 and s[i + 1] == ptree.char and s[i] == child.char:  # transposition\n",
    "            if child.is_word and k == 1 and n == i + 2:\n",
    "                # following transition to grandchild omits the case (in current architecture)\n",
    "                # when child node forms a valid trace, check it (in upper if) and append manually\n",
    "                res.add(child.trace())\n",
    "                if not child.children:\n",
    "                    update_visited(child, visited)\n",
    "\n",
    "            for grandchild in child.children.values():\n",
    "                res.update(fuzzy_match(s, grandchild, k - 1, i + 2, visited, n))\n",
    "\n",
    "        if ptree.char == s[i]:\n",
    "            res.update(fuzzy_match(s, child, k, i + 1, visited, n))  # chars are matched, k remains the same\n",
    "        else:\n",
    "            res.update(fuzzy_match(s, child, k - 1, i + 1, visited, n))  # substitution\n",
    "\n",
    "        res.update(fuzzy_match(s, child, k - 1, i, visited, n))  # delete candidate char\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weightened Damerau-Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_dam_lev(a, b):\n",
    "    d = {}\n",
    "    for i in range(-1, len(a)):\n",
    "        d[(i, -1)] = i + 1\n",
    "    for j in range(-1, len(b)):\n",
    "        d[(-1, j)] = j + 1\n",
    "\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            subst_cost = trans_cost = 0\n",
    "            if a[i] != b[j]:\n",
    "                subst_cost = .8 if b[j] in KEYBOARD[a[i]] or a[i] in KEYBOARD[b[j]] else 1\n",
    "                trans_cost = .9\n",
    "\n",
    "            d[(i, j)] = min([\n",
    "                d[(i - 1, j)] + 1,  # deletion\n",
    "                d[(i, j - 1)] + 1,  # insertion\n",
    "                d[(i - 1, j - 1)] + subst_cost  # substitution\n",
    "            ])\n",
    "\n",
    "            if i and j and a[i] == b[j - 1] and a[i - 1] == b[j]:\n",
    "                d[(i, j)] = min([\n",
    "                    d[(i, j)],\n",
    "                    d[(i - 2, j - 2)] + trans_cost\n",
    "                ])\n",
    "\n",
    "    return d[(len(a) - 1, len(b) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering candidates according to context (within tags N-grams counts) and Damerau-Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_tags(word, morph):\n",
    "    if not word or word in string.punctuation:\n",
    "        return {'PNCT': 1.0}\n",
    "\n",
    "    res = {}\n",
    "    for var in morph.parse(word):\n",
    "        res[','.join(sorted([t for t in RE_S.sub(',', str(var.tag)).split(',') if t not in REDUNDANT_TAGS]))] = var.score\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_ngram_relevance_score(left_t, c_t, right_t, c_t_pool, bi_cnt, tri_cnt):\n",
    "    l_total = r_total = s_total = 0\n",
    "\n",
    "    for t in c_t_pool:\n",
    "        l_total += bi_cnt.get('%s+%s' % (left_t, t), 0)\n",
    "        r_total += bi_cnt.get('%s+%s' % (t, right_t), 0)\n",
    "        s_total += tri_cnt.get('%s+%s+%s' % (left_t, t, right_t), 0)\n",
    "\n",
    "    l = 0 if not l_total else float(bi_cnt.get('%s+%s' % (left_t, c_t), 0)) / l_total\n",
    "    r = 0 if not r_total else float(bi_cnt.get('%s+%s' % (c_t, right_t), 0)) / r_total\n",
    "    s = 0 if not s_total else float(tri_cnt.get('%s+%s+%s' % (left_t, c_t, right_t), 0)) / s_total\n",
    "\n",
    "    return .25 * l + .25 * r + .5 * s\n",
    "\n",
    "\n",
    "def filter_candidates(left, candidates, right, morph, bi_cnt, tri_cnt, debug=True):\n",
    "    \"\"\"\n",
    "    1. Loop through each combination of left-right tags.\n",
    "       On each iteration we are trying to find the most relevant tag among each candidate tag.\n",
    "            2. Relevance is measured as weighted sum of bi/trigram probabilities counted by maximum likelihood\n",
    "               only among candidates tags.\n",
    "            3. Relevance is multiplied on a probability of the candidate to belong to concrete tag.\n",
    "            4. Relevance is multiplied on the probabilities of left/right word tags\n",
    "            5. Compute log of relevance score\n",
    "            6. In case of non-real-word error:\n",
    "                    - multiply relevance score on candidate_dam_lev_distance + 1.\n",
    "               In case of real-word error:\n",
    "                    - add to relevance score:\n",
    "                            - np.log(0.99)                         - for real word\n",
    "                            - np.log(0.01 / (len(candidates) - 1)) - for other words\n",
    "    7. Repeat 2-6 for each combination of left-right tags.\n",
    "    8. Find maximum of relevance scores and argmax (which is the desired candidate).\n",
    "\n",
    "    :param left:            left word or None in case of sentence beginning\n",
    "    :param candidates:      dictionary of candidates {'candidate': candidate_dam_lev_distance}\n",
    "    :param right:           right word or None in case of sentence ending\n",
    "    :param morph:           instance of MorphAnalyzer\n",
    "    :param bi_cnt:          dictionary of tags bigrams counts\n",
    "    :param tri_cnt:         dictionary of tags trigrams counts\n",
    "    :param debug:           whether to display debug information\n",
    "\n",
    "    :return:                most possible candidate or None if no candidates passed\n",
    "    \"\"\"\n",
    "    if len(candidates) < 2:\n",
    "        return None if not candidates else candidates.keys()[0]\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    real_word = min(candidates.values()) == 0\n",
    "    other_word_add = np.log(.01 / (len(candidates) - 1)) if real_word else None\n",
    "    if debug:\n",
    "        print(\"Real-word: %s\" % real_word)\n",
    "\n",
    "    tags_l = get_word_tags(left, morph)\n",
    "    tags_r = get_word_tags(right, morph)\n",
    "    tags_c = {}\n",
    "    c_t_pool = set()  # set-pool of distinct candidates tags\n",
    "\n",
    "    for c in candidates:\n",
    "        tags_c[c] = get_word_tags(c, morph)\n",
    "        for t, score in tags_c[c].items():\n",
    "            c_t_pool.add(t)\n",
    "\n",
    "    for lt in tags_l:\n",
    "        for rt in tags_r:\n",
    "            for c in candidates:\n",
    "                for ct in tags_c[c]:\n",
    "                    rel_score = get_ngram_relevance_score(lt, ct, rt, c_t_pool, bi_cnt, tri_cnt)\n",
    "                    rel_score = np.log(rel_score) if rel_score else -np.inf\n",
    "                    rel_score += np.log(tags_c[c][ct])\n",
    "                    rel_score += np.log(tags_l[lt]) + np.log(tags_r[rt])\n",
    "\n",
    "                    if real_word:\n",
    "                        rel_score += np.log(.99) if candidates[c] == 0 else other_word_add\n",
    "                    else:\n",
    "                        rel_score *= (candidates[c] + 1)\n",
    "\n",
    "                    all_scores['%s:%s|%s:%s|%s:%s' % (left, lt, c, ct, right, rt)] = rel_score\n",
    "\n",
    "    if debug:\n",
    "        print('\\n'.join([str(x) for x in sorted(all_scores.items(), key=lambda x: x[1], reverse=True) if not np.isinf(x[1])]))\n",
    "\n",
    "    if real_word:\n",
    "        # in case of real-word error retrieve all max relevant sequences\n",
    "        # choose the most relevant candidate from them according to min candidate_dam_lev_distance\n",
    "        max_score = all_scores[max(all_scores, key=lambda k: all_scores[k])]\n",
    "        most_relevant_candidates = [seq.split('|')[1].split(':')[0] for seq, score in all_scores.items() if score == max_score]\n",
    "        return min(most_relevant_candidates, key=lambda k: candidates[k])\n",
    "    else:\n",
    "        # in case of non-real-word error find max relevant sequence and retrieve candidate from it\n",
    "        return max(all_scores, key=lambda k: all_scores[k]).split('|')[1].split(':')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrefixTree loading from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 s, sys: 7.36 s, total: 34.2 s\n",
      "Wall time: 41.6 s\n"
     ]
    }
   ],
   "source": [
    "%time pt = load_ptree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts of OpenCorpora tags N-grams loading from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 5.93 ms, total: 20.3 ms\n",
      "Wall time: 31.2 ms\n",
      "CPU times: user 68.8 ms, sys: 17.3 ms, total: 86.1 ms\n",
      "Wall time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "%time tags_bi = json.load(open('bigram.opcorpora.json', encoding='utf-8'))\n",
    "%time tags_tri = json.load(open('trigram.opcorpora.json', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMorphy2 MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.7 ms, sys: 30.9 ms, total: 98.7 ms\n",
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%time ma = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing filter_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-word: False\n",
      "('повести:NOUN,femn,inan,loct,sing|прекраснее:COMP|чем:CONJ', -9.2149746844128444)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекраснее:COMP|чем:CONJ', -9.2149746844128444)\n",
      "('повести:INFN,tran|прекрасные:ADJF,accs,inan,plur|чем:CONJ', -10.893231572956974)\n",
      "('повести:INFN,tran|прекраснее:COMP|чем:CONJ', -11.685130251431023)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекраснее:COMP|чем:NPRO,ablt,neut,sing', -12.605239951490809)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекраснее:COMP|чем:NPRO,loct,neut,sing', -12.605239951490809)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекраснее:COMP|чем:NPRO,ablt,neut,sing', -12.605239951490809)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекраснее:COMP|чем:NPRO,loct,neut,sing', -12.605239951490809)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекраснее:COMP|чем:CONJ', -12.713374394031362)\n",
      "('повести:NOUN,datv,femn,inan,sing|прекраснее:COMP|чем:CONJ', -12.713374394031362)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прекраснее:COMP|чем:CONJ', -12.713374394031362)\n",
      "('повести:VERB,impr,sing,tran|прекраснее:COMP|чем:CONJ', -12.713374394031362)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прекрасные:ADJF,nomn,plur|чем:CONJ', -12.754611895876916)\n",
      "('повести:NOUN,femn,inan,loct,sing|призрачнее:COMP|чем:CONJ', -12.900964558177982)\n",
      "('повести:NOUN,accs,femn,inan,plur|призрачнее:COMP|чем:CONJ', -12.900964558177982)\n",
      "('повести:NOUN,datv,femn,inan,sing|покраснее:COMP,Cmp2|чем:CONJ', -13.292297996828426)\n",
      "('повести:NOUN,datv,femn,inan,sing|прекрасное:ADJF,accs,neut,sing|чем:CONJ', -13.329116730537267)\n",
      "('повести:INFN,tran|прикрасе:NOUN,datv,femn,inan,sing|чем:CONJ', -13.698126240311359)\n",
      "('повести:NOUN,femn,inan,loct,sing|прикладнее:COMP|чем:CONJ', -13.822462026619267)\n",
      "('повести:NOUN,femn,inan,loct,sing|приказнее:COMP|чем:CONJ', -13.822462026619267)\n",
      "('повести:NOUN,accs,femn,inan,plur|прикладнее:COMP|чем:CONJ', -13.822462026619267)\n",
      "('повести:NOUN,accs,femn,inan,plur|приказнее:COMP|чем:CONJ', -13.822462026619267)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасные:ADJF,nomn,plur|чем:CONJ', -14.462149309390359)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекрасные:ADJF,nomn,plur|чем:CONJ', -15.347701258701086)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекрасные:ADJF,nomn,plur|чем:CONJ', -15.347701258701086)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекрасные:ADJF,accs,inan,plur|чем:CONJ', -15.612023061231886)\n",
      "('повести:NOUN,femn,gent,inan,sing|покраснее:COMP,Cmp2|чем:CONJ', -16.354447288868222)\n",
      "('повести:INFN,tran|призрачнее:COMP|чем:CONJ', -16.35918235200343)\n",
      "('повести:NOUN,femn,gent,inan,sing|прикрасе:NOUN,femn,inan,loct,sing|чем:CONJ', -16.422894050451621)\n",
      "('повести:NOUN,datv,femn,inan,sing|прикрасе:NOUN,femn,inan,loct,sing|чем:CONJ', -16.422894050451621)\n",
      "('повести:NOUN,femn,inan,loct,sing|прикрасе:NOUN,femn,inan,loct,sing|чем:CONJ', -16.422894050451621)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прикрасе:NOUN,femn,inan,loct,sing|чем:CONJ', -16.422894050451621)\n",
      "('повести:NOUN,accs,femn,inan,plur|прикрасе:NOUN,femn,inan,loct,sing|чем:CONJ', -16.422894050451621)\n",
      "('повести:INFN,tran|прикрасе:NOUN,femn,inan,loct,sing|чем:CONJ', -16.422894050451621)\n",
      "('повести:VERB,impr,sing,tran|прикрасе:NOUN,femn,inan,loct,sing|чем:CONJ', -16.422894050451621)\n",
      "('повести:INFN,tran|прекраснее:COMP|чем:NPRO,ablt,neut,sing', -16.516017737510523)\n",
      "('повести:INFN,tran|прекраснее:COMP|чем:NPRO,loct,neut,sing', -16.516017737510523)\n",
      "('повести:INFN,tran|прекрасное:ADJF,accs,neut,sing|чем:CONJ', -16.654795309808307)\n",
      "('повести:NOUN,femn,gent,inan,sing|прикрасе:NOUN,datv,femn,inan,sing|чем:CONJ', -17.384079860025874)\n",
      "('повести:INFN,tran|прикладнее:COMP|чем:CONJ', -17.527695377146536)\n",
      "('повести:INFN,tran|приказнее:COMP|чем:CONJ', -17.527695377146536)\n",
      "('повести:NOUN,femn,inan,loct,sing|прикрасьте:VERB,impr,plur,tran|чем:CONJ', -17.53758471974907)\n",
      "('повести:NOUN,datv,femn,inan,sing|покраснее:COMP,Cmp2|чем:NPRO,ablt,neut,sing', -17.647335932087131)\n",
      "('повести:NOUN,datv,femn,inan,sing|покраснее:COMP,Cmp2|чем:NPRO,loct,neut,sing', -17.647335932087131)\n",
      "('повести:NOUN,femn,inan,loct,sing|призрачнее:COMP|чем:NPRO,ablt,neut,sing', -17.647335932087131)\n",
      "('повести:NOUN,femn,inan,loct,sing|призрачнее:COMP|чем:NPRO,loct,neut,sing', -17.647335932087131)\n",
      "('повести:NOUN,accs,femn,inan,plur|призрачнее:COMP|чем:NPRO,ablt,neut,sing', -17.647335932087131)\n",
      "('повести:NOUN,accs,femn,inan,plur|призрачнее:COMP|чем:NPRO,loct,neut,sing', -17.647335932087131)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прекрасные:ADJF,nomn,plur|чем:NPRO,ablt,neut,sing', -17.691464602911722)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прекрасные:ADJF,nomn,plur|чем:NPRO,loct,neut,sing', -17.691464602911722)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекрасное:ADJF,neut,nomn,sing|чем:CONJ', -17.744566334211427)\n",
      "('повести:NOUN,femn,gent,inan,sing|призрачнее:COMP|чем:CONJ', -17.798724151643906)\n",
      "('повести:NOUN,datv,femn,inan,sing|призрачнее:COMP|чем:CONJ', -17.798724151643906)\n",
      "('повести:NOUN,femn,inan,nomn,plur|призрачнее:COMP|чем:CONJ', -17.798724151643906)\n",
      "('повести:VERB,impr,sing,tran|призрачнее:COMP|чем:CONJ', -17.798724151643906)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасные:ADJF,accs,inan,plur|чем:CONJ', -17.845344486074374)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасное:ADJF,neut,nomn,sing|чем:CONJ', -17.955373921751093)\n",
      "('повести:NOUN,datv,femn,inan,sing|прекрасное:ADJF,accs,neut,sing|чем:NPRO,ablt,neut,sing', -18.044816709882372)\n",
      "('повести:NOUN,datv,femn,inan,sing|прекрасное:ADJF,accs,neut,sing|чем:NPRO,loct,neut,sing', -18.044816709882372)\n",
      "('повести:INFN,tran|прекрасные:ADJF,accs,inan,plur|чем:NPRO,ablt,neut,sing', -18.769586605635411)\n",
      "('повести:INFN,tran|прекрасные:ADJF,accs,inan,plur|чем:NPRO,loct,neut,sing', -18.769586605635411)\n",
      "('повести:NOUN,datv,femn,inan,sing|прекрасные:ADJF,nomn,plur|чем:CONJ', -18.829933468026432)\n",
      "('повести:INFN,tran|прекрасные:ADJF,nomn,plur|чем:CONJ', -18.829933468026432)\n",
      "('повести:VERB,impr,sing,tran|прекрасные:ADJF,nomn,plur|чем:CONJ', -18.829933468026432)\n",
      "('повести:NOUN,femn,inan,loct,sing|прикладнее:COMP|чем:NPRO,ablt,neut,sing', -18.907859927236213)\n",
      "('повести:NOUN,femn,inan,loct,sing|приказнее:COMP|чем:NPRO,ablt,neut,sing', -18.907859927236213)\n",
      "('повести:NOUN,femn,inan,loct,sing|прикладнее:COMP|чем:NPRO,loct,neut,sing', -18.907859927236213)\n",
      "('повести:NOUN,femn,inan,loct,sing|приказнее:COMP|чем:NPRO,loct,neut,sing', -18.907859927236213)\n",
      "('повести:NOUN,accs,femn,inan,plur|прикладнее:COMP|чем:NPRO,ablt,neut,sing', -18.907859927236213)\n",
      "('повести:NOUN,accs,femn,inan,plur|приказнее:COMP|чем:NPRO,ablt,neut,sing', -18.907859927236213)\n",
      "('повести:NOUN,accs,femn,inan,plur|прикладнее:COMP|чем:NPRO,loct,neut,sing', -18.907859927236213)\n",
      "('повести:NOUN,accs,femn,inan,plur|приказнее:COMP|чем:NPRO,loct,neut,sing', -18.907859927236213)\n",
      "('повести:NOUN,datv,femn,inan,sing|прикрасе:NOUN,datv,femn,inan,sing|чем:CONJ', -18.947595607486278)\n",
      "('повести:NOUN,femn,inan,loct,sing|прикрасе:NOUN,datv,femn,inan,sing|чем:CONJ', -18.947595607486278)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прикрасе:NOUN,datv,femn,inan,sing|чем:CONJ', -18.947595607486278)\n",
      "('повести:NOUN,accs,femn,inan,plur|прикрасе:NOUN,datv,femn,inan,sing|чем:CONJ', -18.947595607486278)\n",
      "('повести:VERB,impr,sing,tran|прикрасе:NOUN,datv,femn,inan,sing|чем:CONJ', -18.947595607486278)\n",
      "('повести:INFN,tran|покраснее:COMP,Cmp2|чем:CONJ', -18.990728808968839)\n",
      "('повести:NOUN,femn,gent,inan,sing|прикладнее:COMP|чем:CONJ', -19.070061591047043)\n",
      "('повести:NOUN,femn,gent,inan,sing|приказнее:COMP|чем:CONJ', -19.070061591047043)\n",
      "('повести:NOUN,datv,femn,inan,sing|прикладнее:COMP|чем:CONJ', -19.070061591047043)\n",
      "('повести:NOUN,datv,femn,inan,sing|приказнее:COMP|чем:CONJ', -19.070061591047043)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прикладнее:COMP|чем:CONJ', -19.070061591047043)\n",
      "('повести:NOUN,femn,inan,nomn,plur|приказнее:COMP|чем:CONJ', -19.070061591047043)\n",
      "('повести:VERB,impr,sing,tran|прикладнее:COMP|чем:CONJ', -19.070061591047043)\n",
      "('повести:VERB,impr,sing,tran|приказнее:COMP|чем:CONJ', -19.070061591047043)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасные:ADJF,nomn,plur|чем:NPRO,ablt,neut,sing', -19.770906144591557)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасные:ADJF,nomn,plur|чем:NPRO,loct,neut,sing', -19.770906144591557)\n",
      "('повести:NOUN,datv,femn,inan,sing|прекрасные:ADJF,accs,inan,plur|чем:CONJ', -19.770906144591557)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекрасные:ADJF,accs,inan,plur|чем:CONJ', -19.770906144591557)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прекрасные:ADJF,accs,inan,plur|чем:CONJ', -19.770906144591557)\n",
      "('повести:VERB,impr,sing,tran|прекрасные:ADJF,accs,inan,plur|чем:CONJ', -19.770906144591557)\n",
      "('повести:INFN,tran|прекрасное:ADJF,neut,nomn,sing|чем:CONJ', -20.892011825179036)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекрасные:ADJF,nomn,plur|чем:NPRO,ablt,neut,sing', -20.987301468916051)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекрасные:ADJF,nomn,plur|чем:NPRO,loct,neut,sing', -20.987301468916051)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекрасные:ADJF,nomn,plur|чем:NPRO,ablt,neut,sing', -20.987301468916051)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекрасные:ADJF,accs,inan,plur|чем:NPRO,ablt,neut,sing', -20.987301468916051)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекрасные:ADJF,nomn,plur|чем:NPRO,loct,neut,sing', -20.987301468916051)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекрасные:ADJF,accs,inan,plur|чем:NPRO,loct,neut,sing', -20.987301468916051)\n",
      "('повести:NOUN,femn,gent,inan,sing|покраснее:COMP,Cmp2|чем:NPRO,ablt,neut,sing', -21.018459784199752)\n",
      "('повести:NOUN,femn,gent,inan,sing|покраснее:COMP,Cmp2|чем:NPRO,loct,neut,sing', -21.018459784199752)\n",
      "('повести:NOUN,datv,femn,inan,sing|прекрасное:ADJF,neut,nomn,sing|чем:CONJ', -21.149506132728376)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прекрасное:ADJF,neut,nomn,sing|чем:CONJ', -21.149506132728376)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекрасное:ADJF,neut,nomn,sing|чем:CONJ', -21.149506132728376)\n",
      "('повести:VERB,impr,sing,tran|прекрасное:ADJF,neut,nomn,sing|чем:CONJ', -21.149506132728376)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасное:ADJF,accs,neut,sing|чем:CONJ', -21.502855239697531)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекрасное:ADJF,accs,neut,sing|чем:CONJ', -21.502855239697531)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прекрасное:ADJF,accs,neut,sing|чем:CONJ', -21.502855239697531)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекрасное:ADJF,accs,neut,sing|чем:CONJ', -21.502855239697531)\n",
      "('повести:VERB,impr,sing,tran|прекрасное:ADJF,accs,neut,sing|чем:CONJ', -21.502855239697531)\n",
      "('повести:NOUN,femn,inan,loct,sing|покраснее:COMP,Cmp2|чем:CONJ', -21.680348362779601)\n",
      "('повести:NOUN,femn,inan,nomn,plur|покраснее:COMP,Cmp2|чем:CONJ', -21.680348362779601)\n",
      "('повести:NOUN,accs,femn,inan,plur|покраснее:COMP,Cmp2|чем:CONJ', -21.680348362779601)\n",
      "('повести:VERB,impr,sing,tran|покраснее:COMP,Cmp2|чем:CONJ', -21.680348362779601)\n",
      "('повести:INFN,tran|прекрасное:ADJF,accs,neut,sing|чем:NPRO,ablt,neut,sing', -21.831541847232106)\n",
      "('повести:INFN,tran|прекрасное:ADJF,accs,neut,sing|чем:NPRO,loct,neut,sing', -21.831541847232106)\n",
      "('повести:NOUN,femn,inan,loct,sing|прикрасьте:VERB,impr,plur,tran|чем:NPRO,ablt,neut,sing', -22.203696793240542)\n",
      "('повести:NOUN,femn,inan,loct,sing|прикрасьте:VERB,impr,plur,tran|чем:NPRO,loct,neut,sing', -22.203696793240542)\n",
      "('повести:INFN,tran|призрачнее:COMP|чем:NPRO,ablt,neut,sing', -23.122424832514731)\n",
      "('повести:INFN,tran|призрачнее:COMP|чем:NPRO,loct,neut,sing', -23.122424832514731)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекрасное:ADJF,neut,nomn,sing|чем:NPRO,ablt,neut,sing', -23.420095117566539)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекрасное:ADJF,neut,nomn,sing|чем:NPRO,loct,neut,sing', -23.420095117566539)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасное:ADJF,neut,nomn,sing|чем:NPRO,ablt,neut,sing', -23.736176664540015)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасное:ADJF,neut,nomn,sing|чем:NPRO,loct,neut,sing', -23.736176664540015)\n",
      "('повести:INFN,tran|покраснее:COMP,Cmp2|чем:NPRO,ablt,neut,sing', -24.552736579059506)\n",
      "('повести:INFN,tran|покраснее:COMP,Cmp2|чем:NPRO,loct,neut,sing', -24.552736579059506)\n",
      "('повести:NOUN,femn,gent,inan,sing|прикрасе:NOUN,datv,femn,inan,sing|чем:NPRO,ablt,neut,sing', -24.599219881893859)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасные:ADJF,accs,inan,plur|чем:NPRO,ablt,neut,sing', -24.599219881893859)\n",
      "('повести:NOUN,femn,gent,inan,sing|прикрасе:NOUN,datv,femn,inan,sing|чем:NPRO,loct,neut,sing', -24.599219881893859)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасные:ADJF,accs,inan,plur|чем:NPRO,loct,neut,sing', -24.599219881893859)\n",
      "('повести:INFN,tran|прикладнее:COMP|чем:NPRO,ablt,neut,sing', -24.774026606265785)\n",
      "('повести:INFN,tran|приказнее:COMP|чем:NPRO,ablt,neut,sing', -24.774026606265785)\n",
      "('повести:INFN,tran|прикладнее:COMP|чем:NPRO,loct,neut,sing', -24.774026606265785)\n",
      "('повести:INFN,tran|приказнее:COMP|чем:NPRO,loct,neut,sing', -24.774026606265785)\n",
      "('повести:NOUN,femn,gent,inan,sing|прекрасней:COMP,V-ej|чем:CONJ', -26.524781540411041)\n",
      "('повести:NOUN,femn,gent,inan,sing|прикрасьте:VERB,impr,plur,tran|чем:CONJ', -26.524781540411041)\n",
      "('повести:NOUN,datv,femn,inan,sing|прекрасней:COMP,V-ej|чем:CONJ', -26.524781540411041)\n",
      "('повести:NOUN,datv,femn,inan,sing|прикрасьте:VERB,impr,plur,tran|чем:CONJ', -26.524781540411041)\n",
      "('повести:NOUN,femn,inan,loct,sing|прекрасней:COMP,V-ej|чем:CONJ', -26.524781540411041)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прекрасней:COMP,V-ej|чем:CONJ', -26.524781540411041)\n",
      "('повести:NOUN,femn,inan,nomn,plur|прикрасьте:VERB,impr,plur,tran|чем:CONJ', -26.524781540411041)\n",
      "('повести:NOUN,accs,femn,inan,plur|прекрасней:COMP,V-ej|чем:CONJ', -26.524781540411041)\n",
      "('повести:NOUN,accs,femn,inan,plur|прикрасьте:VERB,impr,plur,tran|чем:CONJ', -26.524781540411041)\n",
      "('повести:INFN,tran|прекрасней:COMP,V-ej|чем:CONJ', -26.524781540411041)\n",
      "('повести:INFN,tran|прикрасьте:VERB,impr,plur,tran|чем:CONJ', -26.524781540411041)\n",
      "('повести:VERB,impr,sing,tran|прекрасней:COMP,V-ej|чем:CONJ', -26.524781540411041)\n",
      "('повести:VERB,impr,sing,tran|прикрасьте:VERB,impr,plur,tran|чем:CONJ', -26.524781540411041)\n",
      "('повести:INFN,tran|прикрасе:NOUN,datv,femn,inan,sing|чем:NPRO,ablt,neut,sing', -28.385945019243593)\n",
      "('повести:INFN,tran|прикрасе:NOUN,datv,femn,inan,sing|чем:NPRO,loct,neut,sing', -28.385945019243593)\n",
      "('повести:INFN,tran|прекрасное:ADJF,neut,nomn,sing|чем:NPRO,ablt,neut,sing', -32.898180209573916)\n",
      "('повести:INFN,tran|прекрасное:ADJF,neut,nomn,sing|чем:NPRO,loct,neut,sing', -32.898180209573916)\n",
      "\n",
      "Best candidate: прекраснее\n",
      "Execution time: 0.28774309158325195s\n"
     ]
    }
   ],
   "source": [
    "misspelled = 'прикраснее'\n",
    "st = time()\n",
    "filtered = filter_candidates(\n",
    "    'повести',\n",
    "    {c: weighted_dam_lev(c, misspelled) for c in fuzzy_match(misspelled, pt, MAX_DISTANCE)},\n",
    "    'чем',\n",
    "    ma,\n",
    "    tags_bi,\n",
    "    tags_tri\n",
    ")\n",
    "print(\"\\nBest candidate: %s\\nExecution time: %ss\" % (filtered, time() - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical to text conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'двух', 'одной тысячи девятисот девяноста семи']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RE_CASE = OrderedDict()\n",
    "RE_CASE['gent'] = re.compile('^(\\d+)-?\\w*((о?го)|((у|е|ё)?х)|и|а)$')\n",
    "RE_CASE['datv'] = re.compile('^(\\d+)-?\\w*((о?му)|((у|е|ё)?м)|и|а)$')\n",
    "RE_CASE['ablt'] = re.compile('^(\\d+)-?\\w*(((у|е)?мя)|[тм]?ь?ю)$')\n",
    "RE_CASE['loct'] = re.compile('^(\\d+)-?\\w*(((у|е|ё)?х)|([тм]?и))$')\n",
    "\n",
    "\n",
    "def case_for_numerical(text, case):\n",
    "    return ' '.join([ma.parse(w)[0].inflect({case})[0] for w in text.split()])\n",
    "\n",
    "\n",
    "def replace(text):\n",
    "    for case, regex in RE_CASE.items():\n",
    "        match = regex.search(text)\n",
    "        if match:\n",
    "            numerical = pytils.numeral.in_words(int(match.group(1)))\n",
    "            return case_for_numerical(numerical, case)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def numbers2letters(words):\n",
    "    return [replace(w) if w[0] in string.digits else w for w in words]\n",
    "\n",
    "\n",
    "numbers2letters(['1', '2-ух', '1997-ми'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and rule-based corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "russian_vowels = ['а', 'у', 'о', 'ы', 'и', 'э', 'я', 'ю', 'ё', 'е']\n",
    "russian_vowels_str = ''.join(russian_vowels)\n",
    "russian_cons = ['б', 'в', 'г', 'д', 'ж', 'з', 'й', 'к', 'л', 'м', 'н', 'п', 'р', 'с', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ']\n",
    "russian_cons_str = ''.join(russian_cons)\n",
    "\n",
    "extra_whitespace = re.compile('\\s+', re.U)\n",
    "repeated_chars = re.compile('([а-яa-z])\\1\\1+', re.U)\n",
    "tsa_ending = re.compile('(.+)(цца|ццо)$', re.U)\n",
    "vobsch_reg = re.compile('в(о|а){1,2}(б|п)щ{1,2}(е|и)м', re.U)\n",
    "potomy_reg = re.compile('п(о|а)т(о|а)му\\s?(ч|ш)т(о|а)', re.U)\n",
    "\n",
    "frequent_intentional_mistakes = {'собстно': 'собственно', 'собсна': 'собственно', \"многабуков\": \"много букв\",\n",
    "                                 \"седня\": \"сегодня\", \"естесно\": \"естественно\", \"ессно\": \"естественно\",\n",
    "                                 \"естессно\": \"естественно\", \"ничо\": \"ничего\", \"неоч\": \"не очень\", \"щаз\": \"сейчас\",\n",
    "                                 \"какбы\": \"как бы\", \"какбе\": \"как бы\", \"скока\": \"сколько\", \"нащщот\": \"насчет\",\n",
    "                                 \"ваще\": \"вообще\", \"ващще\": \"вообще\"}\n",
    "\n",
    "frequent_hyphen_space_mistakes = {\"изза\": \"из-за\", \"еслиб\": \"если б\", \"тоесть\": \"то есть\", \"всмысле\": \"в смысле\",\n",
    "                                  \"такчто\": \"так что\"}\n",
    "\n",
    "hyphen_endings = ['либо', 'нибудь', \"то\"]\n",
    "subj_particles = [\"кто\", \"как\", \"если\", \"когда\", \"вот\", \"хоть\", \"пусть\"]\n",
    "hyphen_beg = [\"вице\", \"камер\", \"контр\", \"лейб\", \"обер\", \"статс\", \"унтер\", \"флигель\", \"штаб\", \"штабс\", \"экс\"]\n",
    "\n",
    "\n",
    "def tokenize(text, punct_include=False):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if punct_include:\n",
    "        return tokens\n",
    "\n",
    "    return [i for i in tokens if i not in string.punctuation]\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # at first, replace most popular mistakes\n",
    "    text = vobsch_reg.sub(\" в общем \", text)\n",
    "    text = potomy_reg.sub(\" потому что \", text)\n",
    "\n",
    "    text = extra_whitespace.sub(' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('_', ' ')\n",
    "    text = text.replace('...', '.')\n",
    "    text = text.replace('ё', 'е')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def neighborhood(iterable):\n",
    "    \"\"\"\n",
    "    Iterator wrapper, which gives access to prev and next elements\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    prev = None\n",
    "    curr = next(iterator)\n",
    "\n",
    "    for nxt in iterator:\n",
    "        yield (prev, curr, nxt)\n",
    "        prev = curr\n",
    "        curr = nxt\n",
    "\n",
    "    yield (prev, curr, None)\n",
    "\n",
    "\n",
    "def correct_hyphens_spaces(words):\n",
    "    corrected = []\n",
    "    skip_next = False\n",
    "\n",
    "    for prev, word, nxt in neighborhood(words):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        n_corrected = len(corrected)\n",
    "\n",
    "        # проверяем \"не\" с глаголом\n",
    "        if word.startswith('не'):\n",
    "            if ma.parse(word[2:])[0].tag.POS == 'VERB':\n",
    "                corrected.append('не')\n",
    "                corrected.append(word[2:])\n",
    "                continue\n",
    "\n",
    "        if word in frequent_hyphen_space_mistakes:\n",
    "            word = frequent_hyphen_space_mistakes[word]\n",
    "            if ' ' in word:\n",
    "                # если одно слово заменили на несколько\n",
    "                for sub_word in word.split(' '):\n",
    "                    corrected.append(sub_word)\n",
    "                continue\n",
    "\n",
    "        if word.endswith(\"бы\"):\n",
    "            if word[:-2] in subj_particles:\n",
    "                corrected.append(word[:-2])\n",
    "                corrected.append(\"бы\")\n",
    "                continue\n",
    "\n",
    "        # дефисы\n",
    "        if '-' not in word:\n",
    "            # кое-, кой-\n",
    "            if word.startswith('кое') or word.startswith(\"кой\"):\n",
    "                # если приставку написали как отдельное слово - соединяем со следующим\n",
    "                if len(word) == 3:\n",
    "                    if len(nxt) > 1:\n",
    "                        corrected.append(word + \"-\" + nxt)\n",
    "                        skip_next = True\n",
    "                        continue\n",
    "                else:\n",
    "                    # если часть слова после приставки есть в нашем словаре - вставляем дефис\n",
    "                    if word[3:] in pt:\n",
    "                        corrected.append(word[:3] + \"-\" + word[3:])\n",
    "                        continue\n",
    "\n",
    "            # вице-, камер-, контр-, лейб-, обер-, статс-, унтер-, флигель-, штабс- и экс-\n",
    "            for beg in hyphen_beg:\n",
    "                if word.startswith(beg):\n",
    "                    corrected.append(\"{0}-{1}\".format(beg, word[len(beg):]))\n",
    "                    continue\n",
    "            # пол-\n",
    "            if word.startswith('пол'):\n",
    "                if word[3:].startswith('л'):\n",
    "                    corrected.append(\"пол-{0}\".format(word[3:]))\n",
    "                    continue\n",
    "\n",
    "            # -либо, -нибудь, -то\n",
    "            for ending in hyphen_endings:\n",
    "                if word.endswith(ending):\n",
    "                    # вставляем дефис если часть слова без окончания есть в словаре\n",
    "                    first_part = word[:-len(ending)]\n",
    "                    if first_part in pt:\n",
    "                        corrected.append(\"{0}-{1}\".format(first_part, ending))\n",
    "\n",
    "        # если за эту итерацию мы еще не добавляли слов - добавляем исходное\n",
    "        if n_corrected == len(corrected):\n",
    "            corrected.append(word)\n",
    "\n",
    "    return corrected\n",
    "\n",
    "\n",
    "def correct_intentional_misspelling(words):\n",
    "    corrected = []\n",
    "    for word in words:\n",
    "        if word not in pt:\n",
    "            # все повторяющиеся символы (от 3 и более) заменяются на один\n",
    "            word = repeated_chars.sub('\\\\1', word)\n",
    "\n",
    "            if word in frequent_intentional_mistakes:\n",
    "                word = frequent_intentional_mistakes[word]\n",
    "                if ' ' in word:\n",
    "                    # если одно слово заменили на несколько\n",
    "                    for sub_word in word.split(' '):\n",
    "                        corrected.append(sub_word)\n",
    "                    continue\n",
    "                # заменяем окончания типа -цца и -ццо\n",
    "                word = tsa_ending.sub('\\\\1ться', word)\n",
    "\n",
    "        corrected.append(word)\n",
    "\n",
    "    return corrected\n",
    "\n",
    "\n",
    "def preprocess_text(text, punct_include=False):\n",
    "    text = clean_text(text)\n",
    "\n",
    "    words = tokenize(text, punct_include)\n",
    "    words = numbers2letters(words)\n",
    "    words = correct_intentional_misspelling(words)\n",
    "    words = correct_hyphens_spaces(words)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def correct_sentence(sent, pt, morph, bi_cnt, tri_cnt, real_word=False):\n",
    "#     tokens = preprocess_text(sent)\n",
    "\n",
    "#     for i in range(len(tokens)):\n",
    "#         if real_word or tokens[i] not in pt:\n",
    "#             corrected = filter_candidates(\n",
    "#                 tokens[i - 1] if i > 0 else '',\n",
    "#                 {c: weighted_dam_lev(c, tokens[i]) for c in fuzzy_match(tokens[i], pt, MAX_DISTANCE)},\n",
    "#                 tokens[i + 1] if i + 1 < len(tokens) else '',\n",
    "#                 morph,\n",
    "#                 bi_cnt,\n",
    "#                 tri_cnt\n",
    "#             )\n",
    "#             if corrected:\n",
    "#                 tokens[i] = corrected\n",
    "    \n",
    "#     return tokens"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
